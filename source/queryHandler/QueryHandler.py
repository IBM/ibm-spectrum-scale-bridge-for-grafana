'''
##############################################################################
# Copyright 2019 IBM Corp.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################

Created on Feb 4, 2017

@author: NSCHULD
'''

import json
import operator
import socket
import time
import analytics
from collections import namedtuple, defaultdict
from itertools import chain
from typing import NamedTuple, Tuple
from utils import cond_execution_time

from .PerfmonRESTclient import perfHTTPrequestHelper, createRequestDataObj, getAuthHandler


class PerfmonConnError(Exception):
    pass


EMPTY = json.loads('{"header" : {"bcount" : 0, "bsize" : 0, "t_start" : 0, "t_end" : 0 }, "legend" : [], "rows" : [], "rangeData": [] }')

HeaderData = namedtuple('HeaderData', 'bcount, bsize, t_start, t_end')
'''structure for header data           int,   int,   long,   long '''


class Row(namedtuple('_Row', 'tstamp, values, nsamples')):
    '''structure for a row of data long, [numbers], [numbers]'''
    __slots__ = ()

    @property
    def time_str(self):
        return time.strftime("%Y-%m-%d_%H:%M:%S", time.localtime(self.tstamp))

    def is_empty(self):
        return sum(self.nsamples) == 0
        # return len(self.values) == self.values.count(None)


class Key(namedtuple('_Key', 'parent, sensor, identifier, metric, domains')):
    '''data structure of a Key string, string, tuple of strings (can be empty), string, tuple of domains
    describing parent (node or cluster), sensor and metric for the data
    as well as the identifier if the metric applies to multiple items.
    Also joined are the aggregation domains of that key which describe the effective bucket size'''
    __slots__ = ()

    @classmethod
    def _from_string(cls, key, domains):
        '''Split a string like node.localnet.com|Network|eth0|netdev_bytes_s to its consisting parts'''
        items = key.split('|')
        return Key(items[0], items[1], tuple(items[2:-1]), items[-1], domains)

    def __str__(self):
        return '|'.join([self.parent, self.sensor, '|'.join(self.identifier), self.metric]).replace('||', '|')

    def shortKey_str(self):
        return '|'.join([self.parent, self.sensor, '|'.join(self.identifier)])

    def __repr__(self):
        return self.__str__()

    def __hash__(self):
        return hash((self.parent, self.sensor, self.identifier, self.metric))

    def __eq__(self, other):
        return (self.parent, self.sensor, self.identifier, self.metric) == (other.parent, other.sensor, other.identifier, other.metric)

    def __ne__(self, other):
        return not (self == other)


class ColumnInfo(NamedTuple):
    '''header data for each column          string, int, [KEY], int '''
    name: str
    semType: int
    keys: Tuple[Key]
    column: int

    @property
    def key_str(self):
        '''string which includes all keys'''
        return ', '.join(str(key) for key in self.keys if key)

    @property
    def identifiers(self):
        ''' get all identifiers from all Keys'''
        ident = set(key.identifier for key in self.flat_keys if key.identifier)
        if len(ident) == 1:
            return ident.pop()
        return tuple(ident)

    @property
    def parents(self):
        ''' get parent(s) of this column'''
        parent = set(key.parent for key in self.flat_keys if key.parent)
        if len(parent) == 1:
            return parent.pop()
        return tuple(parent)

    @property
    def flat_keys(self):
        '''for computed columns we have multiple lists of keys, flatten it to a simple list'''
        if len(self.keys) > 1 or isinstance(self.keys[0], list):
            flat_keys = tuple(chain.from_iterable(self.keys))
            if not isinstance(flat_keys[0], Key):
                flat_keys = self.keys
        else:
            flat_keys = self.keys
        return flat_keys

    def __hash__(self):
        return hash((self.name, self.keys))

    def __eq__(self, other):
        return (self.name, self.keys) == (other.name, other.keys)

    def __ne__(self, other):
        return not (self == other)


class Domain(namedtuple('_domain', 'domainID, start, end, bucketSize')):
    '''data structure of a domain,     int,   int,    int,  int'''
    __slots__ = ()

    @property
    def start_str(self):
        return time.strftime("%Y-%m-%d_%H:%M:%S", time.localtime(self.start))

    @property
    def end_str(self):
        return time.strftime("%Y-%m-%d_%H:%M:%S", time.localtime(self.end))


DEFAULT_DOMAIN = Domain(99, 0, 6666666666, 666)


class QueryResult:
    '''Wrapper for the data returned by a Zimon query'''

    def __init__(self, query, res_json):
        self.query = query
        self.json = res_json

        self.header = self.__parseHeader()
        self.columnInfos = self.__parseLegend()
        self.rows = self.__parseRows()

        self.index_cache = {}  # (metric, id) -> row value index
        self.ids = None

        if self.query and len(self.query.measurements) > 0:
            self.ids = self._findIdentifiers()
            self._populate_index_cache()
            self._add_calculated_colunm_headers()

            calc = Calculator()
            for row in self.rows:
                self._add_calculated_row_data(calc, row)

    def __parseHeader(self):
        item = self.json['header']
        return HeaderData(**item)

    def __parseLegend(self):
        legendItems = self.json['legend']
        columnInfos = []

        domains_by_key = defaultdict(list)
        if "rangedata" in self.json:
            for item in self.json["rangeData"]:
                dkey = item.get('key')
                for domain in item.get('domains'):
                    domains_by_key[dkey].append(Domain(**domain))
                if len(domains_by_key[dkey]) == 0:
                    domains_by_key[dkey].append(DEFAULT_DOMAIN)

        for count, item in enumerate(legendItems):
            keys = item['keys']
            parsedKeys = tuple(Key._from_string(key, tuple(domains_by_key.get(key, [DEFAULT_DOMAIN]))) for key in keys)
            col_info = ColumnInfo(item['caption'], item['semType'], parsedKeys, count)
            columnInfos.append(col_info)
        return columnInfos

    def __parseRows(self):
        return [Row(**item) for item in self.json['rows']]

    def _findIdentifiers(self):
        ids = {}  # using dict as a ordered set, order matters!
        for ci in self.columnInfos:
            p = set(key.parent for key in ci.keys)
            if len(p) == 1:
                parents = p.pop()
            else:
                parents = tuple(p)
            id_item = (parents, ci.identifiers)
            ids[id_item] = 1
        return ids.keys()

    def _add_calculated_colunm_headers(self):
        '''for each measurement create a result column for each ID '''
        nextidx = max(ci.column for ci in self.columnInfos)

        for q_name, prg in self.query.measurements.items():
            metrics = [step for step in prg if step not in Calculator.OPS and not step.isnumeric()]
            for parent, myid in self.ids:
                key_aq = []
                for metric in metrics:
                    idx = self.index_cache.get((metric, parent, myid), -1)
                    if idx != -1:
                        key_aq.extend(key for key in self.columnInfos[idx].keys if key)
                nextidx += 1
                self.columnInfos.append(ColumnInfo(q_name, 15, (tuple(key_aq)), nextidx))

    def _add_calculated_row_data(self, calc, row):
        '''for each measurement calculate result column for each ID for the given row'''
        for parent, myid in self.ids:
            for q_name, prg in self.query.measurements.items():
                calc.clear()
                for step in prg:
                    if step in Calculator.OPS:
                        calc.op(step)
                    elif step.isnumeric():                  # is_number(step):
                        calc.push(float(step))
                    else:
                        idx = self.index_cache.get((step, parent, myid), -1)
                        if idx != -1:
                            value = row.values[idx]
                            if value is None:
                                calc.clear()
                                calc.push(None)
                                break
                            calc.push(value)
                        else:
                            raise ValueError('prg step not identified %s', step)
                res = calc.pop()
                row.values.append(res)

    def __getitem__(self, index):
        return self.rows[index]

    def _populate_index_cache(self):
        for ci in self.columnInfos:
            key = (ci.keys[0].metric, ci.parents, ci.identifiers)
            # if key in self.index_cache:
            #     SysmonLogger.getLogger(__name__).error("hash collision in _populate_index_cache")
            self.index_cache[key] = ci.column

    def drop_base_metrics(self):
        '''remove all headers and data columns which were used to compute a measurement'''
        columns = sorted(self.index_cache.values(), reverse=True)
        for col_idx in columns:
            del self.columnInfos[col_idx]
            for row in self.rows:
                del row.values[col_idx]

        renumberedColumns = []
        for idx, col in enumerate(self.columnInfos):
            renumberedColumns.append(ColumnInfo(col.name, col.semType, col.keys, idx))
        self.columnInfos = renumberedColumns

        return self

    def remove_rows_with_no_data(self):
        self.rows = [r for r in self.rows if not r.is_empty()]

    def reduce(self):
        # self.remove_rows_with_no_data()
        values = [self.latest(col) for col in self.columnInfos]
        ts = self.rows[-1].tstamp if len(self.rows) > 0 else int(time.time())
        return Row(ts, values, [1] * len(values))

    def check_rows_have_no_data(self):
        return True if (len([r for r in self.rows if not r.is_empty()]) == 0) else False

    def latest(self, column):
        ''' get last non null value of a column'''
        return self.__colstat(column, lambda x: next(iter(x)), reverse=True)

    def min(self, column):
        ''' get minimum value of a column'''
        return self.__colstat(column, min)

    def downsampleResults(self, interval, aggregator='avg'):
        ''' Performs downsampling of QueryResult.rows with specified aggregation method and interval'''
        try:
            func = __builtins__[aggregator]
            return self.__downsample(func, int(interval))
        except Exception:
            return self.__downsample(self.dAVG, int(interval))

    def max(self, column):
        ''' get maximum value of a column'''
        return self.__colstat(column, max)

    def sum(self, column):
        ''' get sum of values of a column'''
        return self.__colstat(column, sum)

    def avg(self, column):
        ''' get average value of column'''
        s = self.sum(column)
        l = self.__colstat(column, len)
        return s / (1.0 * l)

    def dAVG(self, valList):
        ''' get average value of values'''
        s = sum(valList)
        l = len(valList)
        if l == 0:
            return None
        return int(round(s / l))

    def __colstat(self, column, fn, reverse=False):
        if isinstance(column, ColumnInfo):
            idx = column.column
        else:
            idx = column
        if reverse:
            data = (row.values[idx] for row in reversed(self.rows))
        else:
            data = (row.values[idx] for row in self.rows)
        try:
            return fn([x for x in data if x is not None])
        except Exception:
            return None

    def __downsample(self, fn, interval, column='all'):

        aggrRows = []

        for i in range(0, len(self.rows), interval):
            rows_chunk = self.rows[i:i + interval]
            chunk_values = [row.values for row in rows_chunk]
            aggr_values = [None] * len(self.columnInfos)
            # iterate through each column of the time interval data chunk
            for idx, column_values in enumerate(zip(*chunk_values)):
                try:
                    if len(column_values) == column_values.count(None):
                        aggr_value = None
                    else:
                        aggr_value = fn([x for x in column_values if x is not None])
                except Exception:
                    aggr_value = None
                aggr_values[idx] = aggr_value

            tIdx = (i + len(chunk_values)) - 1
            aggrRows.append({"tstamp": self.rows[tIdx].tstamp, "values": aggr_values, "nsamples": len(rows_chunk)})

        return [Row(**item) for item in aggrRows]


def is_number(s):
    try:
        float(s)
        return True
    except ValueError:
        pass


def div(a, b):  # defined anew because of py 2/3 difference
    try:
        return float(a) / float(b)
    except:
        return 0


class Calculator(object):
    '''simple UPN calculator'''
    OPS = {"+": operator.add, "-": operator.sub, '*': operator.mul, '/': div, ">=": operator.ge, ">": operator.gt, "<=": operator.le, "<": operator.lt, "==": operator.eq}

    def __init__(self):
        self.stack = []

    def push(self, arg):
        self.stack.append(arg)
        return self

    def pop(self):
        return self.stack.pop()

    def clear(self):
        self.stack.clear()

    def op(self, operation):
        operation = Calculator.OPS[operation]

        a = self.stack.pop()
        b = self.stack.pop()
        c = operation(b, a)
        self.push(c)
        return self


class QueryHandler2:
    '''
    Interface class to access ZIMon data
    '''

    def __init__(self, server, port, logger, apiKeyName, apiKeyValue, caCert=False):
        '''
        Constructor requires name (or IP address) of the server and the port number (default: 9084)
        '''
        self.__keyName = apiKeyName
        self.__keyValue = apiKeyValue
        self.__caCert = caCert
        self.server = server
        self.remote_ip = socket.gethostbyname(server)
        self.port = port
        self.logger = logger

    @property
    def apiKeyData(self):
        return self.__keyName, self.__keyValue

    @property
    def caCert(self):
        return self.__caCert

    @cond_execution_time(enabled=analytics.inspect)
    def getTopology(self, ignoreMetrics=False):
        '''
        Returns complete topology as a single JSON string
        ignoreAttrs can be used to skip the (leaf) metrics
        '''
        params = None
        if ignoreMetrics:
            params = {'query': '-a'}
        res = self.__do_RESTCall('perfmon/topo', 'GET', params)

        if res is None:
            self.logger.error("QueryHandler: getTopology returns no data.")
            return
        try:
            result = json.loads(res, strict=False)
            return result
        except Exception as e:
            self.logger.error(
                'QueryHandler: getTopology response not valid json: {0} {1}'.format(res[:20], e))

    def getAvailableMetrics(self):
        '''
        Returns output from topo -m
        '''
        params = {'query': '-m'}
        return self.__do_RESTCall('perfmon/topo', 'GET', params)

    def deleteKeyFromTopology(self, keyStr, precheck=True):
        '''
        Executes the delete command for the given key
        Returns result dictionary
        '''
        # delete pre-check option
        check = '-n' if precheck == True else ''
        deleteString = f'delete {check} key {keyStr} \n'

        params = {'query': deleteString}
        response = self.__do_RESTCall('perfmon/delete', 'DELETE', params)

        if response is None:
            self.logger.debug('QueryHandler: deleteKeysFromTopology response has no data results')
            return None
        try:
            result = json.loads(response, strict=False)
            return result
        except Exception as e:
            self.logger.error(
                'QueryHandler: deleteKeysFromTopology response not valid json: {0} {1}'.format(response[:20], e))

    @cond_execution_time(enabled=analytics.inspect)
    def runQuery(self, query):
        '''
        runQuery: executes the given query based on the arguments.
        :param query: a query class instance
        '''
        params = {'query': str(query)}
        self.logger.trace('QueryHandler: REST call perfmon/data invoked with following params: {0}'.format(params))
        res = self.__do_RESTCall('perfmon/data', 'GET', params)

        if res is None:
            self.logger.debug('QueryHandler: query response has no data results')
            return None
        try:
            result = json.loads(res, strict=False)
            return QueryResult(query, result)
        except Exception as e:
            self.logger.error(
                'QueryHandler: query response not valid json: {0} {1}'.format(res[:20], e))

    def __do_RESTCall(self, endpoint, requestType='GET', params=None):
        '''
        Forward query request to the HTTPRequest client interface
        '''

        # self.logger.trace("__do_RESTcall invoke __ params: {} {} {}".format(endpoint, requestType, str(params)))

        try:
            _auth = getAuthHandler(*self.apiKeyData)
            _reqData = createRequestDataObj(self.logger, requestType, endpoint,
                                            self.server, self.port, auth=_auth,
                                            params=params)
            _request = perfHTTPrequestHelper(self.logger,
                                             reqdata=_reqData,
                                             caCert=self.caCert)
            _response = _request.doRequest()

            if _response.status_code == 200:
                # the r.elapsed is the Time-To-First-Byte (TTFB) while the call to requests.get()
                # only terminates after the whole message has been received (Time-To-Last-Byte, TTLB).
                if analytics.requests_elapsed_time:
                    self.logger.debug(f'response elapsed time: {_response.elapsed.total_seconds()} for request: {str(params)}')
                return _response.content.decode('utf-8', "strict")
            elif _response.status_code == 401:
                self.logger.trace('Request headers:{}'.format(_response.request.headers))
                self.logger.trace('Request url:{}'.format(_response.request.url))
                msg = "Perfmon RESTcall error __ Server responded: {} {}".format(_response.status_code, _response.reason)
                self.logger.details(msg)
                raise PerfmonConnError("{} {}".format(_response.status_code, _response.reason))
            else:
                msg = "Perfmon RESTcall error __ Server responded: {} {}".format(_response.status_code, _response.reason)
                self.logger.details(msg)
                if _response.content:
                    contentMsg = _response.content.decode('utf-8', "strict")
                    self.logger.trace(f'Response content:{contentMsg}')
                return None
        except TypeError as e:
            self.logger.exception(e)
            # return ("", str(e), 500)  # Internal Server Error
            return None
